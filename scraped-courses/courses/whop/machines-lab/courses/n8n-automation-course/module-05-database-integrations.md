# Module 5: Database Integrations & Data Management - Your Data Backbone

**Course:** The Ultimate n8n Course
**Platform:** Whop (Machines Lab)
**URL:** https://whop.com/joined/machines-lab/the-full-n8n-automation-course-qPR2uj0EAxhIOd/app/

---

## üéØ What You'll Master Today

By the end of this module, you'll:

- **Connect** n8n to SQL and NoSQL databases securely
- **Build** data pipelines that sync information across multiple systems
- **Create** automated backup and reporting workflows
- **Develop** a complete customer data warehouse system

---

## üíæ Why Database Integration Changes Everything

Spreadsheets are great for small-scale data, but serious business automation needs proper databases. With database integration, you can:

- Store unlimited customer data with complex relationships
- Run sophisticated queries to analyze business trends
- Create real-time dashboards and reports
- Build scalable systems that grow with your business
- Maintain data integrity across all your tools

Database integration transforms n8n from a simple automation tool into a complete business intelligence platform.

---

## üéØ Database Types and When to Use Each

### SQL Databases (Structured Data)

**MySQL:** Most popular, great for web applications
**PostgreSQL:** Advanced features, excellent for complex queries
**SQLite:** Lightweight, perfect for small projects
**SQL Server:** Enterprise-grade, Microsoft ecosystem

**Best for:** Customer data, order management, financial records, inventory tracking

### NoSQL Databases (Flexible Data)

**MongoDB:** Document-based, handles complex nested data
**Redis:** In-memory, perfect for caching and sessions
**Firebase:** Real-time features, great for mobile apps

**Best for:** User profiles, product catalogs, content management, real-time data

### Cloud Database Services

**Google Cloud SQL:** Managed MySQL/PostgreSQL
**AWS RDS:** Scalable relational databases
**MongoDB Atlas:** Hosted MongoDB service
**Supabase:** PostgreSQL with real-time features

---

## üîó Setting Up Your First Database Connection

Let's connect to a PostgreSQL database to build a customer data system.

### Step 1: Create Database Credentials

- Go to Settings ‚Üí Credentials in n8n
- Add new credential ‚Üí PostgreSQL
- Fill in connection details:
  - **Host:** your-database-host.com
  - **Database:** customer_data
  - **User:** n8n_user
  - **Password:** your-secure-password
  - **Port:** 5432 (default PostgreSQL port)

### Step 2: Test the Connection

- Create a new workflow
- Add a **PostgreSQL** node
- Set Operation to **Execute Query**
- Use this test query:

```sql
SELECT version();
```

- Execute to verify connection works

### Step 3: Create Your Data Schema

Add another PostgreSQL node with this query to create a customers table:

```sql
CREATE TABLE IF NOT EXISTS customers (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    company VARCHAR(200),
    phone VARCHAR(50),
    customer_tier VARCHAR(20) DEFAULT 'standard',
    total_orders INTEGER DEFAULT 0,
    total_spent DECIMAL(10,2) DEFAULT 0.00,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## üìä Building a Customer Data Pipeline

Now let's create a comprehensive system that syncs customer data from multiple sources into your database.

### Step 1: Set Up Multi-Source Data Collection

Create a workflow with a **Schedule Trigger** (runs every 15 minutes) that gathers data from:

- **Shopify Orders** (via HTTP Request):

```sql
-- Query to get recent orders
SELECT customer_email, first_name, last_name,
       COUNT(*) as order_count, SUM(total_price) as total_spent
FROM shopify_orders
WHERE created_at >= NOW() - INTERVAL '15 minutes'
GROUP BY customer_email, first_name, last_name;
```

- **Email Platform** (via HTTP Request): Get subscriber engagement data
- **Support Tickets** (via API): Get customer service interaction history

### Step 2: Data Normalization and Cleaning

Add a **Function** node to standardize data from all sources:

```javascript
// Clean and standardize customer data
const customers = $input.all();
const processedCustomers = [];

customers.forEach(item => {
    const data = item.json;

    // Standardize email format
    const email = data.email ? data.email.toLowerCase().trim() : null;

    // Skip records without valid email
    if (!email || !email.includes('@')) {
        return;
    }

    // Standardize name fields
    const firstName = data.first_name || data.firstName || '';
    const lastName = data.last_name || data.lastName || '';

    // Determine customer tier based on spending
    let tier = 'standard';
    const totalSpent = parseFloat(data.total_spent || 0);
    if (totalSpent > 1000) tier = 'premium';
    else if (totalSpent > 500) tier = 'gold';

    processedCustomers.push({
        email: email,
        first_name: firstName,
        last_name: lastName,
        company: data.company || '',
        phone: data.phone || '',
        customer_tier: tier,
        total_orders: parseInt(data.total_orders || 0),
        total_spent: totalSpent,
        source: data.source || 'unknown'
    });
});

return processedCustomers.map(customer => ({ json: customer }));
```

### Step 3: Upsert Customer Data

Add a **PostgreSQL** node with an UPSERT query (insert or update):

```sql
INSERT INTO customers (email, first_name, last_name, company, phone, customer_tier, total_orders, total_spent, updated_at)
VALUES ($1, $2, $3, $4, $5, $6, $7, $8, CURRENT_TIMESTAMP)
ON CONFLICT (email)
DO UPDATE SET
    first_name = EXCLUDED.first_name,
    last_name = EXCLUDED.last_name,
    company = EXCLUDED.company,
    phone = EXCLUDED.phone,
    customer_tier = EXCLUDED.customer_tier,
    total_orders = EXCLUDED.total_orders,
    total_spent = EXCLUDED.total_spent,
    updated_at = CURRENT_TIMESTAMP;
```

Parameters:

- `$1`: `{{ $json.email }}`
- `$2`: `{{ $json.first_name }}`
- `$3`: `{{ $json.last_name }}`
- `$4`: `{{ $json.company }}`
- `$5`: `{{ $json.phone }}`
- `$6`: `{{ $json.customer_tier }}`
- `$7`: `{{ $json.total_orders }}`
- `$8`: `{{ $json.total_spent }}`

---

## üìà Creating Automated Reports and Analytics

### Daily Customer Report Workflow

Build a daily report that analyzes your customer database:

### Step 1: Query Customer Metrics

Add a **PostgreSQL** node with multiple queries:

```sql
-- New customers today
SELECT COUNT(*) as new_customers_today
FROM customers
WHERE DATE(created_at) = CURRENT_DATE;

-- Customer tier breakdown
SELECT customer_tier, COUNT(*) as count, AVG(total_spent) as avg_spent
FROM customers
GROUP BY customer_tier;

-- Top customers by spending
SELECT email, first_name, last_name, total_spent
FROM customers
ORDER BY total_spent DESC
LIMIT 10;

-- Revenue trends (last 30 days)
SELECT DATE(created_at) as date, COUNT(*) as new_customers
FROM customers
WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY DATE(created_at)
ORDER BY date;
```

### Step 2: Format Report Data

Use a **Function** node to create an HTML report:

```javascript
const metrics = $node["PostgreSQL"].json;

// Build HTML email report
const htmlReport = `
<h2>Daily Customer Report - ${new Date().toDateString()}</h2>

<h3>üìä Key Metrics</h3>
<ul>
<li><strong>New Customers Today:</strong> ${metrics.new_customers_today}</li>
<li><strong>Total Active Customers:</strong> ${metrics.total_customers}</li>
</ul>

<h3>üéØ Customer Tiers</h3>
<table border="1" style="border-collapse: collapse;">
<tr><th>Tier</th><th>Count</th><th>Avg Spent</th></tr>
${metrics.tier_breakdown.map(tier =>
  `<tr><td>${tier.customer_tier}</td><td>${tier.count}</td><td>$${tier.avg_spent}</td></tr>`
).join('')}
</table>

<h3>üí∞ Top 10 Customers</h3>
<ol>
${metrics.top_customers.map(customer =>
  `<li>${customer.first_name} ${customer.last_name} (${customer.email}) - $${customer.total_spent}</li>`
).join('')}
</ol>

<p><em>Generated automatically by your n8n workflow</em></p>
`;

return [{ json: { htmlReport } }];
```

### Step 3: Send Report via Email

Add a **Gmail** node:

- **Subject:** `Daily Customer Report - {{ $now.format('MMM DD, YYYY') }}`
- **HTML Body:** `{{ $json.htmlReport }}`

---

## üîÑ Data Backup and Recovery Automation

### Automated Database Backup

Create a weekly backup workflow:

### Step 1: Export Customer Data

Add a **PostgreSQL** node to export all data:

```sql
SELECT * FROM customers
ORDER BY created_at DESC;
```

### Step 2: Convert to CSV Format

Use a **Function** node:

```javascript
const customers = $json;

// Convert to CSV format
const headers = ['ID', 'Email', 'First Name', 'Last Name', 'Company', 'Phone', 'Tier', 'Total Orders', 'Total Spent', 'Created At'];
const csvRows = [headers.join(',')];

customers.forEach(customer => {
    const row = [
        customer.id,
        customer.email,
        customer.first_name,
        customer.last_name,
        customer.company,
        customer.phone,
        customer.customer_tier,
        customer.total_orders,
        customer.total_spent,
        customer.created_at
    ];
    csvRows.push(row.map(field => `"${field}"`).join(','));
});

return [{
    json: {
        csvData: csvRows.join('\\n'),
        filename: `customer_backup_${new Date().toISOString().split('T')[0]}.csv`
    }
}];
```

### Step 3: Save to Cloud Storage

Add a **Google Drive** node to save the backup:

- **Operation:** Upload
- **File Name:** `{{ $json.filename }}`
- **File Content:** `{{ $json.csvData }}`

---

## üí™ Practice Prompts

### Prompt 1: Order Analytics Dashboard (25 minutes)

Build a system that:

- Connects to your e-commerce database
- Tracks daily order volumes, revenue, and average order value
- Identifies best-selling products and categories
- Creates a daily analytics email with charts and insights
- Stores historical data for trend analysis

### Prompt 2: Customer Lifecycle Automation (20 minutes)

Create workflows that:

- Monitor customer database for milestone events (first purchase, 6 months without purchase, etc.)
- Automatically trigger appropriate email campaigns
- Update customer segments based on behavior
- Track campaign effectiveness in the database

### Prompt 3: Data Quality Monitor (15 minutes)

Build a data quality system that:

- Scans your customer database for duplicate emails, missing required fields, invalid data formats
- Generates a data quality report showing issues found
- Automatically fixes simple issues (formatting phone numbers, standardizing names)
- Alerts you to manual review items

---

## üö® Troubleshooting Database Issues

**Connection Failures**
- Verify database credentials are correct
- Check network connectivity and firewall settings
- Ensure database server is running and accessible
- Test connection outside n8n first

**Query Errors**
- Validate SQL syntax in a database client first
- Check that referenced tables and columns exist
- Verify data types match what you're inserting
- Use parameterized queries to prevent SQL injection

**Performance Issues**
- Add indexes on frequently queried columns
- Limit result sets with LIMIT clauses
- Use WHERE clauses to filter data efficiently
- Consider breaking large operations into smaller batches

**Data Integrity Problems**
- Use transactions for multi-step operations
- Implement proper error handling
- Validate data before database operations
- Use foreign key constraints to maintain relationships

---

## üõ°Ô∏è Database Security Best Practices

### Connection Security

- Always use SSL/TLS connections
- Create dedicated database users with minimal permissions
- Use strong, unique passwords for database accounts
- Restrict database access to specific IP addresses when possible

### Data Protection

- Never store sensitive data in plain text
- Implement proper backup encryption
- Use parameterized queries to prevent SQL injection
- Regularly audit database access logs

### Credential Management

- Store all credentials securely in n8n's credential system
- Rotate database passwords regularly
- Use environment variables for connection strings
- Never hardcode credentials in workflows

---

## üìö Essential Resources

- **PostgreSQL Node Guide:** [n8n PostgreSQL Documentation](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.postgresql/) - Complete setup and usage
- **MySQL Integration:** [MySQL Node Reference](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.mysql/) - MySQL-specific configuration
- **MongoDB Guide:** [MongoDB Node Documentation](https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-base.mongodb/) - NoSQL database integration
- **SQL Learning:** [SQLBolt Interactive Tutorial](https://sqlbolt.com/) - Learn SQL fundamentals
- **Database Design:** [Database Design Basics](https://support.microsoft.com/en-us/office/database-design-basics-eb2159cf-1e30-401a-8084-bd4f9c9ca1f5) - Schema design principles
- **Community Templates:** [Database Workflows](https://n8n.io/workflows/categories/databases/) - Real-world database automations

---

## üîÆ Coming Up in Module 6

You now have the power to manage data at scale! In Module 6, we'll dive into **email marketing automation and customer communications**:

- Building sophisticated email sequences and drip campaigns
- Creating personalized content based on customer data
- Setting up behavioral triggers and segmentation
- Integrating with major email platforms (Mailchimp, SendGrid, etc.)
- A/B testing and performance optimization

We'll build a complete email marketing system that automatically nurtures leads, re-engages customers, and drives revenue through intelligent, data-driven communications.

---

‚úÖ **Module 5 Complete!** You can now build scalable data systems that grow with your business. Your automations have a solid data foundation for sophisticated business intelligence. Ready to master email marketing automation in Module 6?
